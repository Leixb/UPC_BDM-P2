import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Encoders;

import scala.Tuple2;

import java.util.Arrays;
import java.util.List;


public class Exploitation {

    // KPI #NUM ACTES CONTRA LA PROPIETAT PRIVADA BY district
    // private static Dataset<Row> compute_and_save_kpi1(SparkSession spark, JavaRDD<Row> formatted){
    //   List<Tuple2<String,Long>> kpi1 = formatted
    //                   .mapToPair(f -> new Tuple2<String, Long>(f.getString(f.fieldIndex("district")), f.getLong(f.fieldIndex("incidents"))))
    //                   .reduceByKey((f1,f2) -> f1+f2)
    //                   .collect();
    //
    //   Dataset<Row> ds_kpi1 = spark.createDataset(kpi1, Encoders.tuple(Encoders.STRING(),Encoders.LONG()))
    //                               .toDF("district","SumIncidents");
    //   ds_kpi1.show();
    //   return ds_kpi1;
    // }

    // KPI incidents/listings by district
    private static Dataset<Row> compute_and_save_kpi1(SparkSession spark, JavaRDD<Row> formatted){
      List<Tuple2<String,Double>> kpi1 = formatted
                      .mapToPair(f -> new Tuple2<String, Tuple2<Double,Integer>>(f.getString(f.fieldIndex("district")),
                                                                                  new Tuple2<Double,Integer>((double)f.getLong(f.fieldIndex("incidents")),Integer.valueOf(1))))
                      .reduceByKey((f1,f2) -> new Tuple2<Double,Integer>(f1._1,f1._2+f2._2))
                      .mapValues(x -> x._1/x._2)
                      .collect();

      Dataset<Row> ds_kpi1 = spark.createDataset(kpi1, Encoders.tuple(Encoders.STRING(),Encoders.DOUBLE()))
                                  .toDF("district","incidents/listings");
      ds_kpi1.show();
      return ds_kpi1;
    }

    // KPI AVERAGE rooms+bathrooms BY propertyType
    private static Dataset<Row> compute_and_save_kpi2(SparkSession spark, JavaRDD<Row> formatted){
      List<Tuple2<String,Double>> kpi2 = formatted
                          .mapToPair(f -> new Tuple2<String,Tuple2<Double,Integer>>(f.getString(f.fieldIndex("propertyType")),
                                                                                    new Tuple2<Double,Integer>(Double.parseDouble(f.getLong(f.fieldIndex("rooms"))+"")+Double.parseDouble(f.getLong(f.fieldIndex("bathrooms"))+""),
                                                                                    Integer.valueOf(1))))
                          .reduceByKey((f1,f2) -> new Tuple2<Double,Integer>(f1._1+f2._1,f1._2+f2._2))
                          .mapValues(x -> x._1/x._2)
                          .collect();

      Dataset<Row> ds_kpi2 = spark.createDataset(kpi2, Encoders.tuple(Encoders.STRING(),Encoders.DOUBLE()))
                                  .toDF("propertyType","AverageRoomsAndBathrooms");
      ds_kpi2.show();
      return ds_kpi2;
    }

    // KPI AVERAGE SQUARE METER PRICE BY neighborhood
    // private static Dataset<Row> compute_and_save_kpi3(SparkSession spark, JavaRDD<Row> formatted){
    //   List<Tuple2<String,Double>> kpi3 = formatted
    //                       .mapToPair(f -> new Tuple2<String,Tuple2<Double,Integer>>(f.getString(f.fieldIndex("neighborhood")),
    //                                         new Tuple2<Double,Integer>(f.getDouble(f.fieldIndex("price"))/f.getDouble(f.fieldIndex("size")),Integer.valueOf(1))))
    //                       .reduceByKey((f1,f2) -> new Tuple2<Double,Integer>(f1._1+f2._1,f1._2+f2._2))
    //                       .mapValues(x -> x._1/x._2)
    //                       .collect();
    //   Dataset<Row> ds_kpi3 = spark.createDataset(kpi3, Encoders.tuple(Encoders.STRING(),Encoders.DOUBLE()))
    //                               .toDF("neighborhood","AverageSqmPrice");
    //   ds_kpi3.show();
    //   return ds_kpi3;
    // }

    // KPI AVERAGE SQUARE METER PRICE BY neighborhood and district
    private static Dataset<Row> compute_and_save_kpi3(SparkSession spark, JavaRDD<Row> formatted){
      List<Tuple2<Tuple2<String,String>,Double>> kpi3 = formatted
                          .mapToPair(f -> new Tuple2<Tuple2<String,String>,Tuple2<Double,Integer>>(new Tuple2<String,String>(f.getString(f.fieldIndex("district")),f.getString(f.fieldIndex("neighborhood"))),
                                            new Tuple2<Double,Integer>(f.getDouble(f.fieldIndex("price"))/f.getDouble(f.fieldIndex("size")),Integer.valueOf(1))))
                          .reduceByKey((f1,f2) -> new Tuple2<Double,Integer>(f1._1+f2._1,f1._2+f2._2))
                          .mapValues(x -> x._1/x._2)
                          .collect();
      Dataset<Row> ds_kpi3 = spark.createDataset(kpi3, Encoders.tuple(Encoders.tuple(Encoders.STRING(),Encoders.STRING()),Encoders.DOUBLE()))
                                  .toDF("neighborhood","kpi3");
      ds_kpi3.show();
      return ds_kpi3;
    }




    public static void run(String[] args) {
        SparkSession spark = SparkSession
                .builder()
                .master("local[*]")
                .appName("P2")
                .getOrCreate();
        spark.sparkContext().setLogLevel("WARN");
        // Parquet data sources
        JavaRDD<Row> formatted = spark.read().parquet("./formatted_zone/*").toJavaRDD();



        String output_dir = "./exploitation_zone";
        compute_and_save_kpi1(spark,formatted).write().parquet(output_dir+"/kpi1");
        compute_and_save_kpi2(spark,formatted).write().parquet(output_dir+"/kpi2");
        compute_and_save_kpi3(spark,formatted).write().parquet(output_dir+"/kpi3");

        // ds_kpi1.write().parquet(output_dir);



        // String out3 = "";
        // KPI AVERAGE SQUARE METER PRICE BY neighborhood

        // out3 += Arrays.toString(kpi3.toArray());
        // System.out.println(out3);






    }
}
