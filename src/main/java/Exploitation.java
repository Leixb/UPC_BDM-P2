import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.SaveMode;

import scala.Tuple2;

import java.util.Arrays;
import java.util.List;
import java.util.ArrayList;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;

import java.util.Collection;
import java.util.Map;
import java.util.HashMap;

import com.google.common.collect.Iterators;
import org.apache.spark.streaming.api.java.JavaPairDStream;

public class Exploitation {

    // KPI #NUM ACTES CONTRA LA PROPIETAT PRIVADA BY district
    // private static Dataset<Row> compute_and_save_kpi1(SparkSession spark, JavaRDD<Row> formatted){
    //   List<Tuple2<String,Long>> kpi1 = formatted
    //                   .mapToPair(f -> new Tuple2<String, Long>(f.getString(f.fieldIndex("district")), f.getLong(f.fieldIndex("incidents"))))
    //                   .reduceByKey((f1,f2) -> f1+f2)
    //                   .collect();
    //
    //   Dataset<Row> ds_kpi1 = spark.createDataset(kpi1, Encoders.tuple(Encoders.STRING(),Encoders.LONG()))
    //                               .toDF("district","SumIncidents");
    //   ds_kpi1.show();
    //   return ds_kpi1;
    // }



    // KPI incidents/listings by district
    private static Dataset<Row> compute_and_save_kpi1(SparkSession spark, JavaRDD<Row> formatted){
      List<Tuple2<String,Double>> kpi1 = formatted
                      .mapToPair(f -> new Tuple2<String, Tuple2<Double,Integer>>(f.getString(f.fieldIndex("district")),
                                                                                  new Tuple2<Double,Integer>((double)f.getLong(f.fieldIndex("incidents")),Integer.valueOf(1))))
                      .reduceByKey((f1,f2) -> new Tuple2<Double,Integer>(f1._1,f1._2+f2._2))
                      .mapValues(x -> x._1/x._2)
                      .collect();

      Dataset<Row> ds_kpi1 = spark.createDataset(kpi1, Encoders.tuple(Encoders.STRING(),Encoders.DOUBLE()))
                                  .toDF("district","incidents/listings");
      ds_kpi1.show();
      return ds_kpi1;
    }

    // KPI AVERAGE rooms+bathrooms BY propertyType
    private static Dataset<Row> compute_and_save_kpi2(SparkSession spark, JavaRDD<Row> formatted){
      List<Tuple2<String,Double>> kpi2 = formatted
                          .mapToPair(f -> new Tuple2<String,Tuple2<Double,Integer>>(f.getString(f.fieldIndex("propertyType")),
                                                                                    new Tuple2<Double,Integer>(Double.parseDouble(f.getLong(f.fieldIndex("rooms"))+"")+Double.parseDouble(f.getLong(f.fieldIndex("bathrooms"))+""),
                                                                                    Integer.valueOf(1))))
                          .reduceByKey((f1,f2) -> new Tuple2<Double,Integer>(f1._1+f2._1,f1._2+f2._2))
                          .mapValues(x -> x._1/x._2)
                          .collect();

      Dataset<Row> ds_kpi2 = spark.createDataset(kpi2, Encoders.tuple(Encoders.STRING(),Encoders.DOUBLE()))
                                  .toDF("propertyType","AverageRoomsAndBathrooms");
      ds_kpi2.show();
      return ds_kpi2;
    }

    // KPI AVERAGE SQUARE METER PRICE BY neighborhood
    private static Dataset<Row> compute_and_save_kpi3(SparkSession spark, JavaRDD<Row> formatted){
      List<Tuple2<String,Double>> kpi3 = formatted
                          .mapToPair(f -> new Tuple2<String,Tuple2<Double,Integer>>(f.getString(f.fieldIndex("neighborhood_id")),
                                            new Tuple2<Double,Integer>(f.getDouble(f.fieldIndex("price"))/f.getDouble(f.fieldIndex("size")),Integer.valueOf(1))))
                          .reduceByKey((f1,f2) -> new Tuple2<Double,Integer>(f1._1+f2._1,f1._2+f2._2))
                          .mapValues(x -> x._1/x._2)
                          .collect();
      Dataset<Row> ds_kpi3 = spark.createDataset(kpi3, Encoders.tuple(Encoders.STRING(),Encoders.DOUBLE()))
                                  .toDF("neighborhood","AverageSqmPrice");
      ds_kpi3.show();
      return ds_kpi3;
    }

    // KPI AVERAGE SQUARE METER PRICE BY neighborhood and district
    // private static Dataset<Row> compute_and_save_kpi3(SparkSession spark, JavaRDD<Row> formatted){
    //   List<Tuple2<Tuple2<String,String>,Double>> kpi3 = formatted
    //                       .mapToPair(f -> new Tuple2<Tuple2<String,String>,Tuple2<Double,Integer>>(new Tuple2<String,String>(f.getString(f.fieldIndex("district")),f.getString(f.fieldIndex("neighborhood_id"))),
    //                                         new Tuple2<Double,Integer>(f.getDouble(f.fieldIndex("price"))/f.getDouble(f.fieldIndex("size")),Integer.valueOf(1))))
    //                       .reduceByKey((f1,f2) -> new Tuple2<Double,Integer>(f1._1+f2._1,f1._2+f2._2))
    //                       .mapValues(x -> x._1/x._2)
    //                       .collect();
    //   Dataset<Row> ds_kpi3 = spark.createDataset(kpi3, Encoders.tuple(Encoders.tuple(Encoders.STRING(),Encoders.STRING()),Encoders.DOUBLE()))
    //                               .toDF("neighborhood","kpi3");
    //   ds_kpi3.show();
    //   return ds_kpi3;
    // }

    // KPI #NUM ACTES CONTRA LA PROPIETAT PRIVADA BY neighborhood
    private static Dataset<Row> compute_and_save_kpi4(SparkSession spark, JavaRDD<Row> formatted){
      List<Tuple2<String,Long>> kpi4 = formatted
                      .mapToPair(f -> new Tuple2<String, Long>(f.getString(f.fieldIndex("neighborhood_id")), f.getLong(f.fieldIndex("incidents"))))
                      .reduceByKey((f1,f2) -> f1)
                      .collect();

      Dataset<Row> ds_kpi4 = spark.createDataset(kpi4, Encoders.tuple(Encoders.STRING(),Encoders.LONG()))
                                  .toDF("neighborhood","incidents");
      ds_kpi4.show();
      return ds_kpi4;
    }

      private static Map<String, Object> createKafkaParamsMap() {
          Map<String,Object> myMap = new HashMap<String,Object>();
          myMap.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "venomoth.fib.upc.edu:9092");
      		myMap.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, org.apache.kafka.common.serialization.StringDeserializer.class);
      		myMap.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, org.apache.kafka.common.serialization.StringDeserializer.class);
      		myMap.put(ConsumerConfig.GROUP_ID_CONFIG, "gid");
          return myMap;
      }

    	private static Collection<String> topics = Arrays.asList("bdm_p2");


    public static void run(String[] args) {
        SparkSession spark = SparkSession
                .builder()
                .master("local[*]")
                .appName("P2")
                .getOrCreate();
        spark.sparkContext().setLogLevel("WARN");
        // Parquet data sources
        JavaRDD<Row> formatted = spark.read().parquet("./formatted_zone/*").toJavaRDD();

        //Compute and save KPIs
        String output_dir = "./exploitation_zone";
        compute_and_save_kpi1(spark,formatted).coalesce(1).write().mode(SaveMode.Overwrite).csv(output_dir+"/kpi1"); //write().parquet(output_dir+"/kpi1");
        compute_and_save_kpi2(spark,formatted).coalesce(1).write().mode(SaveMode.Overwrite).csv(output_dir+"/kpi2");//write().parquet(output_dir+"/kpi2");
        compute_and_save_kpi3(spark,formatted).coalesce(1).write().mode(SaveMode.Overwrite).csv(output_dir+"/kpi3");//write().parquet(output_dir+"/kpi3");
        compute_and_save_kpi4(spark,formatted).coalesce(1).write().mode(SaveMode.Overwrite).csv(output_dir+"/kpi4");//write().parquet(output_dir+"/kpi3");

        spark.close();

        System.out.println("Exploitation Done");

        SparkConf conf = new SparkConf().setAppName("P2").setMaster("local[*]");
    		JavaStreamingContext ssc = new JavaStreamingContext(conf, new Duration(1000));
    		Logger.getRootLogger().setLevel(Level.ERROR);

        ssc.checkpoint("./checkpoint");

    		JavaInputDStream<ConsumerRecord<String, String>> kafkaStream = KafkaUtils.createDirectStream(
    				ssc,
    				LocationStrategies.PreferConsistent(),
    				ConsumerStrategies.Subscribe(topics, createKafkaParamsMap())
    		);

    		// The key is not defined in our stream, ignore it
    		JavaDStream<String> stream = kafkaStream.map(t -> t.value());
    		//stream.print();
        JavaDStream<String> records = stream.flatMap(record -> Iterators.forArray(record.split(",")));
        JavaDStream<String> neighborhoods = records.filter(neighborhood -> neighborhood.startsWith("Q"));
        neighborhoods.print();



        // Count of each neighborhood
        JavaPairDStream<String, Integer> neighborhoodsWith1 = neighborhoods.
                mapToPair(neighborhood -> new Tuple2<>(neighborhood,1));

        JavaPairDStream<String, Integer> counts = neighborhoodsWith1.
                reduceByKeyAndWindow(
                        (i1,i2) -> i1+i2,
                        (i1,i2) -> i1-i2,
                        new Duration(60 * 5 * 1000),
                        new Duration(1 * 1000));

        JavaPairDStream<Integer, String> swappedCounts = counts.mapToPair(count -> count.swap());
        JavaPairDStream<Integer, String> sortedCounts = swappedCounts.transformToPair(count -> count.sortByKey(false));

        sortedCounts.foreachRDD(rdd -> {
          String out = "\nCountedNeighborhood:\n";
                            for (Tuple2<Integer, String> t : rdd.take(20)) {
                                out = out + t.toString() + "\n";
                            }
                            System.out.println(out);
                        });
    		ssc.start();

        try {
    		ssc.awaitTermination();}
        catch(Exception e) {
          System.exit(0);
        }
        ssc.stop();



    }
}
