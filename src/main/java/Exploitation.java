import java.util.List;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SaveMode;
import org.apache.spark.sql.SparkSession;

import scala.Tuple2;

public class Exploitation {

    // KPI #NUM ACTES CONTRA LA PROPIETAT PRIVADA BY district
    // private static Dataset<Row> compute_and_save_kpi1(SparkSession spark, JavaRDD<Row> formatted){
    //   List<Tuple2<String,Long>> kpi1 = formatted
    //                   .mapToPair(f -> new Tuple2<String, Long>(f.getString(f.fieldIndex("district")), f.getLong(f.fieldIndex("incidents"))))
    //                   .reduceByKey((f1,f2) -> f1+f2)
    //                   .collect();
    //
    //   Dataset<Row> ds_kpi1 = spark.createDataset(kpi1, Encoders.tuple(Encoders.STRING(),Encoders.LONG()))
    //                               .toDF("district","SumIncidents");
    //   ds_kpi1.show();
    //   return ds_kpi1;
    // }

    // KPI incidents/listings by district
    private static Dataset<Row> compute_and_save_kpi1(SparkSession spark, JavaRDD<Row> formatted) {
        List<Tuple2<String, Double>> kpi1 = formatted
                .mapToPair(f -> new Tuple2<String, Tuple2<Double, Integer>>(f.getString(f.fieldIndex("district")),
                        new Tuple2<Double, Integer>((double) f.getLong(f.fieldIndex("incidents")), Integer.valueOf(1))))
                .reduceByKey((f1, f2) -> new Tuple2<Double, Integer>(f1._1, f1._2 + f2._2))
                .mapValues(x -> x._1 / x._2)
                .collect();

        Dataset<Row> ds_kpi1 = spark.createDataset(kpi1, Encoders.tuple(Encoders.STRING(), Encoders.DOUBLE()))
                .toDF("district", "incidents/listings");
        ds_kpi1.show();
        return ds_kpi1;
    }

    // KPI AVERAGE rooms+bathrooms BY propertyType
    private static Dataset<Row> compute_and_save_kpi2(SparkSession spark, JavaRDD<Row> formatted) {
        List<Tuple2<String, Double>> kpi2 = formatted
                .mapToPair(f -> new Tuple2<String, Tuple2<Double, Integer>>(f.getString(f.fieldIndex("propertyType")),
                        new Tuple2<Double, Integer>(
                                Double.parseDouble(f.getLong(f.fieldIndex("rooms")) + "")
                                        + Double.parseDouble(f.getLong(f.fieldIndex("bathrooms")) + ""),
                                Integer.valueOf(1))))
                .reduceByKey((f1, f2) -> new Tuple2<Double, Integer>(f1._1 + f2._1, f1._2 + f2._2))
                .mapValues(x -> x._1 / x._2)
                .collect();

        Dataset<Row> ds_kpi2 = spark.createDataset(kpi2, Encoders.tuple(Encoders.STRING(), Encoders.DOUBLE()))
                .toDF("propertyType", "AverageRoomsAndBathrooms");
        ds_kpi2.show();
        return ds_kpi2;
    }




    // KPI AVERAGE SQUARE METER PRICE BY neighborhood
    private static Dataset<Row> compute_and_save_kpi3(SparkSession spark, JavaRDD<Row> formatted){
      List<Tuple2<String,Double>> kpi3 = formatted
                          .mapToPair(f -> new Tuple2<String,Tuple2<Double,Integer>>(f.getString(f.fieldIndex("neighborhood")),
                                            new Tuple2<Double,Integer>(f.getDouble(f.fieldIndex("price"))/f.getDouble(f.fieldIndex("size")),Integer.valueOf(1))))
                          .reduceByKey((f1,f2) -> new Tuple2<Double,Integer>(f1._1+f2._1,f1._2+f2._2))
                          .mapValues(x -> x._1/x._2)
                          .collect();
      Dataset<Row> ds_kpi3 = spark.createDataset(kpi3, Encoders.tuple(Encoders.STRING(),Encoders.DOUBLE()))
                                  .toDF("neighborhood","AverageSqmPrice");
      ds_kpi3.show();
      return ds_kpi3;
    }


    // KPI AVERAGE SQUARE METER PRICE BY neighborhood and district
    // private static Dataset<Row> compute_and_save_kpi3(SparkSession spark, JavaRDD<Row> formatted){
    //   List<Tuple2<Tuple2<String,String>,Double>> kpi3 = formatted
    //                       .mapToPair(f -> new Tuple2<Tuple2<String,String>,Tuple2<Double,Integer>>(new Tuple2<String,String>(f.getString(f.fieldIndex("district")),f.getString(f.fieldIndex("neighborhood_id"))),
    //                                         new Tuple2<Double,Integer>(f.getDouble(f.fieldIndex("price"))/f.getDouble(f.fieldIndex("size")),Integer.valueOf(1))))
    //                       .reduceByKey((f1,f2) -> new Tuple2<Double,Integer>(f1._1+f2._1,f1._2+f2._2))
    //                       .mapValues(x -> x._1/x._2)
    //                       .collect();
    //   Dataset<Row> ds_kpi3 = spark.createDataset(kpi3, Encoders.tuple(Encoders.tuple(Encoders.STRING(),Encoders.STRING()),Encoders.DOUBLE()))
    //                               .toDF("neighborhood","kpi3");
    //   ds_kpi3.show();
    //   return ds_kpi3;
    // }

    // KPI #NUM ACTES CONTRA LA PROPIETAT PRIVADA BY neighborhood
    private static Dataset<Row> compute_and_save_kpi4(SparkSession spark, JavaRDD<Row> formatted){
      List<Tuple2<String,Long>> kpi4 = formatted
                      .mapToPair(f -> new Tuple2<String, Long>(f.getString(f.fieldIndex("neighborhood")), f.getLong(f.fieldIndex("incidents"))))
                      .reduceByKey((f1,f2) -> f1)
                      .collect();

      Dataset<Row> ds_kpi4 = spark.createDataset(kpi4, Encoders.tuple(Encoders.STRING(),Encoders.LONG()))
                                  .toDF("neighborhood","incidents");
      ds_kpi4.show();
      return ds_kpi4;
    }



    // KPI #size/(rooms+bathroom) by district

    private static Dataset<Row> compute_and_save_kpi5(SparkSession spark, JavaRDD<Row> formatted) {
        List<Tuple2<String, Double>> kpi5 = formatted
                .mapToPair(f -> new Tuple2<String, Tuple2<Double, Integer>>(f.getString(f.fieldIndex("propertyType")),
                        new Tuple2<Double, Integer>(f.getDouble(f.fieldIndex("size")) / (
                                Double.parseDouble(f.getLong(f.fieldIndex("rooms")) + "")
                                        + Double.parseDouble(f.getLong(f.fieldIndex("bathrooms")) + "")),
                                Integer.valueOf(1))))
                .reduceByKey((f1, f2) -> new Tuple2<Double, Integer>(f1._1 + f2._1, f1._2 + f2._2))
                .mapValues(x -> x._1 / x._2)
                .collect();

        Dataset<Row> ds_kpi5 = spark.createDataset(kpi5, Encoders.tuple(Encoders.STRING(), Encoders.DOUBLE()))
                .toDF("district", "AverageSizeOfRoomsAndBathrooms");
        ds_kpi5.show();
        return ds_kpi5;
    }

    public static void run(String[] args) {
        SparkSession spark = SparkSession
                .builder()
                .master("local[*]")
                .appName("P2")
                .getOrCreate();
        spark.sparkContext().setLogLevel("WARN");
        // Parquet data sources
        JavaRDD<Row> formatted = spark.read().parquet("./formatted_zone/*").toJavaRDD().cache();

        // Compute and save KPIs
        String output_dir = "./exploitation_zone";
        compute_and_save_kpi1(spark, formatted).coalesce(1).write().mode(SaveMode.Overwrite).csv(output_dir + "/kpi1"); // write().parquet(output_dir+"/kpi1");
        compute_and_save_kpi2(spark, formatted).coalesce(1).write().mode(SaveMode.Overwrite).csv(output_dir + "/kpi2");// write().parquet(output_dir+"/kpi2");
        compute_and_save_kpi3(spark, formatted).coalesce(1).write().mode(SaveMode.Overwrite).csv(output_dir + "/kpi3");// write().parquet(output_dir+"/kpi3");
        compute_and_save_kpi4(spark, formatted).coalesce(1).write().mode(SaveMode.Overwrite).csv(output_dir + "/kpi4");// write().parquet(output_dir+"/kpi3");
        compute_and_save_kpi5(spark, formatted).coalesce(1).write().mode(SaveMode.Overwrite).csv(output_dir + "/kpi5");// write().parquet(output_dir+"/kpi3");

        spark.close();

        System.out.println("Exploitation Done");

    }
}
